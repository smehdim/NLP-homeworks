{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install transformers==4.33.1 peft==0.11.1 datasets\n","!pip install bitsandbytes\n","!pip install gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install datasets\n","!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q trl xformers wandb datasets einops gradio sentencepiece bitsandbytes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T11:02:17.104567Z","iopub.status.busy":"2024-06-02T11:02:17.104174Z","iopub.status.idle":"2024-06-02T11:02:22.396789Z","shell.execute_reply":"2024-06-02T11:02:22.395500Z","shell.execute_reply.started":"2024-06-02T11:02:17.104533Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments,\n","import os,torch, wandb, platform, gradio, warnings\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-02T11:11:02.284094Z","iopub.status.busy":"2024-06-02T11:11:02.283359Z","iopub.status.idle":"2024-06-02T11:11:02.306214Z","shell.execute_reply":"2024-06-02T11:11:02.305180Z","shell.execute_reply.started":"2024-06-02T11:11:02.284059Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fd7b183dfcd49cc9cdf59e19ada2279","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["notebook_login()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bnb_config = BitsAndBytesConfig(\n","    load_in_4bit= True,\n","    bnb_4bit_quant_type= \"nf4\",\n","    bnb_4bit_compute_dtype= torch.float16,\n","    bnb_4bit_use_double_quant= False,\n",")\n","model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","notebook_login()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = \"meta-llama/Meta-Llama-3-8B\"\n","\n","dataset_name = \"multi_nli\"\n","\n","dataset = load_dataset(dataset_name)\n","model_name = \"meta-llama/Meta-Llama-3-8B\"\n","\n","dataset_name = \"multi_nli\"\n","\n","dataset = load_dataset(dataset_name)\n","\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit= True,\n","    bnb_4bit_quant_type= \"nf4\",\n","    bnb_4bit_compute_dtype= torch.float16,\n","    bnb_4bit_use_double_quant= False,\n",")\n","\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config= bnb_config,\n","    device_map=\"auto\",\n",")\n","\n","model = prepare_model_for_kbit_training(model)\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T22:31:42.577544Z","iopub.status.busy":"2024-05-30T22:31:42.576756Z","iopub.status.idle":"2024-05-30T22:31:43.088272Z","shell.execute_reply":"2024-05-30T22:31:43.087423Z","shell.execute_reply.started":"2024-05-30T22:31:42.577510Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.add_eos_token = True"]},{"cell_type":"markdown","metadata":{},"source":["### zero shot"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T22:03:59.042271Z","iopub.status.busy":"2024-05-30T22:03:59.041554Z","iopub.status.idle":"2024-05-30T22:05:14.632171Z","shell.execute_reply":"2024-05-30T22:05:14.631184Z","shell.execute_reply.started":"2024-05-30T22:03:59.042231Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.14\n"]}],"source":["from datasets import load_dataset\n","import torch\n","\n","def zero_shot_prompt(premise, hypothesis):\n","    prompt = (\n","        f\"Premise: {premise}\\n\"\n","        f\"Hypothesis: {hypothesis}\\n\"\n","        f\"Does the hypothesis follow from the premise?\\n\"\n","        f\"Answer (yes, no, maybe):\"\n","    )\n","    return prompt\n","\n","def extract_answer(output_text):\n","    possible_answers = [\"yes\", \"no\", \"maybe\"]\n","    for word in output_text.split():\n","        if word.lower() in possible_answers:\n","            return word.lower()\n","    return \"no_ans\"  \n","\n","def predict_nli(model, tokenizer, premise, hypothesis, temperature=0.5):\n","    prompt = zero_shot_prompt(premise, hypothesis)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n","    outputs = model.generate(\n","        **inputs,\n","        max_length=inputs['input_ids'].shape[1] + 10,\n","        temperature=temperature,\n","        do_sample=True\n","    )\n","    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    answer = extract_answer(answer.split(\"Answer (yes, no, maybe):\")[-1].strip())\n","    return answer\n","\n","def evaluate_model(model, tokenizer, dataset, max_samples=50):\n","    correct = 0\n","    total = 0\n","\n","    label_map = {0: \"yes\", 2: \"no\", 1: \"maybe\"}\n","\n","    for example in dataset['validation_matched']:\n","        if total >= max_samples:\n","            break\n","        \n","        premise = example['premise']\n","        hypothesis = example['hypothesis']\n","        gold_label = example['label']\n","        \n","        if gold_label not in label_map:\n","            continue\n","\n","        predicted_label = predict_nli(model, tokenizer, premise, hypothesis)\n","        \n","        if predicted_label == label_map[gold_label]:\n","            correct += 1\n","\n","        total += 1\n","\n","    accuracy = correct / total if total > 0 else 0\n","    return accuracy\n","\n","\n","accuracy = evaluate_model(model, tokenizer, dataset)\n","print(f\"Accuracy: {accuracy:.2f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### one shot"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-30T22:05:39.724542Z","iopub.status.busy":"2024-05-30T22:05:39.724162Z","iopub.status.idle":"2024-05-30T22:06:50.141059Z","shell.execute_reply":"2024-05-30T22:06:50.140162Z","shell.execute_reply.started":"2024-05-30T22:05:39.724510Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Model accuracy with one-shot learning: 0.40\n"]}],"source":["from datasets import load_dataset\n","\n","one_shot_example = (\n","    \"A man inspects the uniform of a figure in some East Asian country.\", \n","    \"The man is looking at something.\",  \n","    \"yes\" \n",")\n","\n","# Function to create a one-shot prompt\n","def one_shot_prompt(premise, hypothesis, example_premise, example_hypothesis, example_label):\n","    prompt = (\n","        f\"Example:\\n\"\n","        f\"Premise: {example_premise}\\n\"\n","        f\"Hypothesis: {example_hypothesis}\\n\"\n","        f\"Label: {example_label}\\n\\n\"\n","        f\"Now, predict the label for the following example:\\n\"\n","        f\"Premise: {premise}\\n\"\n","        f\"Hypothesis: {hypothesis}\\n\"\n","        f\"Does the hypothesis follow from the premise?\\n\"\n","        f\"Answer (yes, no, maybe):\"\n","    )\n","    return prompt\n","\n","# Function to extract answer\n","def extract_answer(output_text):\n","    possible_answers = [\"yes\", \"no\", \"maybe\"]\n","    for word in output_text.split():\n","        if word.lower() in possible_answers:\n","            return word.lower()\n","    return \"no_ans\"  # Default to 'no_ans' if no valid answer is found\n","\n","def predict_nli(model, tokenizer, premise, hypothesis, example, temperature=0.5):\n","    example_premise, example_hypothesis, example_label = example\n","    prompt = one_shot_prompt(premise, hypothesis, example_premise, example_hypothesis, example_label)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n","    outputs = model.generate(\n","        **inputs,\n","        max_length=inputs['input_ids'].shape[1] + 10,\n","        temperature=temperature,\n","        do_sample=True\n","    )\n","    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    answer = extract_answer(answer.split(\"Answer (yes, no, maybe):\")[-1].strip())\n","    return answer\n","\n","# Function to evaluate the model with one-shot learning\n","def evaluate_model(model, tokenizer, dataset, one_shot_example, max_samples=50):\n","    correct = 0\n","    total = 0\n","\n","    label_map = {0: \"yes\", 2: \"no\", 1: \"maybe\"}\n","\n","    for example in dataset['validation_matched']:\n","        if total >= max_samples:\n","            break\n","        \n","        premise = example['premise']\n","        hypothesis = example['hypothesis']\n","        gold_label = example['label']\n","\n","        # Skip examples with labels not in the label_map (e.g., -1 in MultiNLI)\n","        if gold_label not in label_map:\n","            continue\n","\n","        predicted_label = predict_nli(model, tokenizer, premise, hypothesis, one_shot_example)\n","        \n","        if predicted_label == label_map[gold_label]:\n","            correct += 1\n","\n","        total += 1\n","\n","    accuracy = correct / total if total > 0 else 0\n","    return accuracy\n","\n","dataset = load_dataset('multi_nli')\n","\n","\n","accuracy = evaluate_model(model, tokenizer, dataset, one_shot_example)\n","print(f\"Model accuracy with one-shot learning: {accuracy:.2f}\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
