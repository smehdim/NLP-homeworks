{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "652fe0dae2454aa2a0e740ca439f78d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_588a48c47fd04098864f742a6564f068",
              "IPY_MODEL_d4d18d25628146db92d8434941e957d3",
              "IPY_MODEL_1a3ae7b5f79b4f04b84da5809354b8d6"
            ],
            "layout": "IPY_MODEL_283d489c1fb049e388cb63eded742424"
          }
        },
        "49438489d5af4a92a050ac85e3dddfc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa4673bc93d540c5a5260a3aa4beaa9a",
              "IPY_MODEL_925628d146e74f05b503fba99be913a4",
              "IPY_MODEL_3fad3ab55b984a5d8ae56e6d041b914d"
            ],
            "layout": "IPY_MODEL_8978c134fa9a441eb36c42c75bf81921"
          }
        },
        "8740d06a2ff34fa388f698189adcf076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c25073a14904440c94fd7a35ab5157af",
              "IPY_MODEL_144b86a6dcd64936a2e765944b27297c",
              "IPY_MODEL_cc3740e269be436d81562f177930deb0"
            ],
            "layout": "IPY_MODEL_7c5b526446d94fbcb682e938dcb5884c"
          }
        },
        "248a4f45ff814112a11efd82ad9c6aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1632f7c42ce42e0bfec97e6e3d3d223",
              "IPY_MODEL_400a35e80a3f4b419db300f2eb398bf0",
              "IPY_MODEL_fdc5e0e26807418899b21195344c2e72"
            ],
            "layout": "IPY_MODEL_19104c5938a54a59a38e001720ff638c"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxtDzFNWsYAR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "652fe0dae2454aa2a0e740ca439f78d7",
            "49438489d5af4a92a050ac85e3dddfc9",
            "8740d06a2ff34fa388f698189adcf076",
            "248a4f45ff814112a11efd82ad9c6aa7",
            "588a48c47fd04098864f742a6564f068",
            "d4d18d25628146db92d8434941e957d3",
            "1a3ae7b5f79b4f04b84da5809354b8d6",
            "283d489c1fb049e388cb63eded742424",
            "fa4673bc93d540c5a5260a3aa4beaa9a",
            "925628d146e74f05b503fba99be913a4",
            "3fad3ab55b984a5d8ae56e6d041b914d",
            "8978c134fa9a441eb36c42c75bf81921",
            "c25073a14904440c94fd7a35ab5157af",
            "144b86a6dcd64936a2e765944b27297c",
            "cc3740e269be436d81562f177930deb0",
            "7c5b526446d94fbcb682e938dcb5884c",
            "e1632f7c42ce42e0bfec97e6e3d3d223",
            "400a35e80a3f4b419db300f2eb398bf0",
            "fdc5e0e26807418899b21195344c2e72",
            "19104c5938a54a59a38e001720ff638c"
          ]
        },
        "outputId": "d4c177ae-4882-405e-b918-a93b4dd4e4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "652fe0dae2454aa2a0e740ca439f78d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49438489d5af4a92a050ac85e3dddfc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8740d06a2ff34fa388f698189adcf076"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "248a4f45ff814112a11efd82ad9c6aa7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input\n",
        "text = \"Here is some text to encode to encrypt\"\n",
        "encoded_input = tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode tokenized input\n",
        "decoded_text = tokenizer.decode(encoded_input[\"input_ids\"])\n",
        "print(decoded_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shm2zlX1sxeH",
        "outputId": "6922b074-dde0-4aac-e7c7-e9ada9a101dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] here is some text to encode to encrypt [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### az inja\n"
      ],
      "metadata": {
        "id": "z9589poTfEUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = BertWordPieceTokenizer()\n",
        "\n",
        "# Then train it on your corpus\n",
        "paths ='/content/drive/MyDrive/datasets/nlp/ca1/All_Around_the_Moon.txt'\n",
        "tokenizer.train(files=paths, vocab_size=20000, min_frequency=2)\n",
        "\n",
        "# And finally save it somewhere\n",
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "gfrzGstIvcWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('size = ',tokenizer.get_vocab_size())\n",
        "text = \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\"\n",
        "encoded_input = tokenizer.encode(text)\n",
        "print(\"Tokens:\", encoded_input.tokens)\n",
        "sentence = \"This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model.\"\n",
        "encoded = tokenizer.encode(sentence)\n",
        "print(\"Tokens:\", encoded.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqT5VW3lfaMv",
        "outputId": "daa116ff-9df3-4769-d3d7-be12f7abcd17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size =  8759\n",
            "Tokens: ['this', 'darkness', 'is', 'absolutely', 'killing', '!', 'if', 'we', 'ever', 'take', 'this', 'trip', 'again', ',', 'it', 'must', 'be', 'about', 'the', 'time', 'of', 'the', 'sne', '##w', 'moon', '!']\n",
            "Tokens: ['this', 'is', 'a', 'to', '##ken', '##ization', 'task', '.', 'to', '##ken', '##ization', 'is', 'the', 'first', 'step', 'in', 'a', 'n', '##l', '##p', 'pip', '##el', '##ine', '.', 'we', 'will', 'be', 'compar', '##ing', 'the', 'to', '##ken', '##s', 'generated', 'by', 'each', 'to', '##ken', '##ization', 'mode', '##l', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Then train it on your corpus\n",
        "# paths = [\"path/to/your/corpus.txt\"]\n",
        "tokenizer.train(files=paths, vocab_size=20_000, min_frequency=2)\n",
        "\n",
        "# And finally save it somewhere\n",
        "tokenizer.save(\"tokenizer.json\")\n",
        "tokenizer.get_vocab_size()\n",
        "# Tokenize a new sentence using the trained tokenizer\n",
        "sentence = \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\"\n",
        "encoded = tokenizer.encode(sentence)\n",
        "print('size = ',tokenizer.get_vocab_size())\n",
        "print(\"Tokens:\", encoded.tokens)\n",
        "sentence = \"This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model.\"\n",
        "encoded = tokenizer.encode(sentence)\n",
        "print(\"Tokens:\", encoded.tokens)"
      ],
      "metadata": {
        "id": "2NLteyyTvgIw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b269321-1b0b-42ea-fd95-e1f0d7ac44eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size =  11322\n",
            "Tokens: ['This', 'Ġdarkness', 'Ġis', 'Ġabsolutely', 'Ġkilling', '!', 'ĠIf', 'Ġwe', 'Ġever', 'Ġtake', 'Ġthis', 'Ġtrip', 'Ġagain', ',', 'Ġit', 'Ġmust', 'Ġbe', 'Ġabout', 'Ġthe', 'Ġtime', 'Ġof', 'Ġthe', 'Ġs', 'New', 'ĠMoon', '!']\n",
            "Tokens: ['This', 'Ġis', 'Ġa', 'Ġto', 'ken', 'ization', 'Ġtask', '.', 'ĠT', 'oken', 'ization', 'Ġis', 'Ġthe', 'Ġfirst', 'Ġstep', 'Ġin', 'Ġa', 'ĠN', 'L', 'P', 'Ġpip', 'el', 'ine', '.', 'ĠWe', 'Ġwill', 'Ġbe', 'Ġcomp', 'aring', 'Ġthe', 'Ġto', 'k', 'ens', 'Ġgener', 'ated', 'Ġby', 'Ġeach', 'Ġto', 'ken', 'ization', 'Ġmod', 'el', '.']\n"
          ]
        }
      ]
    }
  ]
}